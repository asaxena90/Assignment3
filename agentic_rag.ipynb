{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplified Agentic RAG System with Gemini API & LangGraph\n",
    "\n",
    "This notebook implements a complete Retrieval-Augmented Generation (RAG) system that uses a self-critique loop to refine its answers. \n",
    "\n",
    "**Workflow:**\n",
    "1.  **Retrieve**: Fetches relevant documents from a Pinecone vector database.\n",
    "2.  **Generate**: Creates an initial answer based on the documents.\n",
    "3.  **Critique**: The LLM evaluates its own answer for completeness.\n",
    "4.  **Refine (if needed)**: If the answer is incomplete, it retrieves more information and generates a new, improved answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation\n",
    "\n",
    "First, we install all the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet langgraph langchain-google-genai pinecone-client langchain-pinecone pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration & Setup\n",
    "\n",
    "Import all required modules and configure API keys. This notebook will prompt you to enter your keys securely when you run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import json\n",
    "import time\n",
    "from typing import List, TypedDict\n",
    "\n",
    "# Import LangChain libraries for Google Gemini\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Import Pinecone and LangChain integration\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# Import LangGraph components\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Import LangChain core components\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "print(\"--- ‚öôÔ∏è Configuring Environment ---\")\n",
    "\n",
    "# Securely get API keys using getpass, suitable for local terminals\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google Gemini API Key: \")\n",
    "\n",
    "if \"PINECONE_API_KEY\" not in os.environ:\n",
    "    PINECONE_API_KEY = getpass.getpass(\"Enter your Pinecone API Key: \")\n",
    "else:\n",
    "    PINECONE_API_KEY = os.environ[\"PINECONE_API_KEY\"]\n",
    "    \n",
    "# Define a unique name for your Pinecone index\n",
    "INDEX_NAME = \"agentic-rag-kb\"\n",
    "print(\"‚úÖ API keys are set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing & Indexing\n",
    "\n",
    "Here, we load the `self_critique_loop_dataset.json` file, generate embeddings for its content using the Gemini API, and store the vectors in a Pinecone index for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- üß† Preprocessing & Indexing Knowledge Base ---\")\n",
    "\n",
    "# Load the knowledge base from the JSON file\n",
    "try:\n",
    "    with open('self_critique_loop_dataset.json', 'r') as f:\n",
    "        kb_data = json.load(f)\n",
    "    print(f\"Loaded {len(kb_data)} documents from the knowledge base.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'self_critique_loop_dataset.json' not found. Make sure it's in the same directory.\")\n",
    "    # In a notebook, we might want to stop execution if the file is missing.\n",
    "    # For this example, we'll just print and continue.\n",
    "\n",
    "# Prepare documents for indexing\n",
    "texts = [doc['answer_snippet'] for doc in kb_data]\n",
    "metadatas = [{'doc_id': doc['doc_id'], 'question': doc['question']} for doc in kb_data]\n",
    "ids = [doc['doc_id'] for doc in kb_data]\n",
    "\n",
    "# Initialize the Gemini Embeddings model\n",
    "print(\"Initializing embedding model...\")\n",
    "embeddings_model = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/text-embedding-004\" # Standard Gemini API embedding model\n",
    ")\n",
    "\n",
    "# Initialize Pinecone client and create the index if it doesn't exist\n",
    "print(\"Connecting to Pinecone and setting up index...\")\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "if INDEX_NAME not in pc.list_indexes().names():\n",
    "    print(f\"Index '{INDEX_NAME}' not found. Creating a new one...\")\n",
    "    pc.create_index(\n",
    "        name=INDEX_NAME, \n",
    "        dimension=768, # Dimension for text-embedding-004\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "    while not pc.describe_index(INDEX_NAME).status['ready']:\n",
    "        time.sleep(1)\n",
    "    print(\"Index created successfully.\")\n",
    "else:\n",
    "    print(f\"Index '{INDEX_NAME}' already exists. Reusing it.\")\n",
    "\n",
    "# Upsert the data into the Pinecone index\n",
    "vectorstore = PineconeVectorStore.from_texts(\n",
    "    texts=texts, \n",
    "    embedding=embeddings_model, \n",
    "    metadatas=metadatas,\n",
    "    ids=ids, \n",
    "    index_name=INDEX_NAME\n",
    ")\n",
    "print(\"‚úÖ Knowledge base has been successfully indexed in Pinecone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LangGraph Workflow Definition\n",
    "\n",
    "This is the core of the agent. We define the state, the nodes (functions that perform actions), and the edges (connections and logic) that control the flow of the RAG process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- üèóÔ∏è Building Agentic RAG Workflow ---\")\n",
    "\n",
    "# Define the state that will be passed between nodes in the graph\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    generation: str\n",
    "    critique: str\n",
    "\n",
    "# Initialize the LLM and Retriever\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\", temperature=0)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={'k': 5})\n",
    "\n",
    "### DEFINE GRAPH NODES ###\n",
    "\n",
    "def retrieve_kb(state):\n",
    "    \"\"\"Retrieves documents from the vector database based on the question.\"\"\"\n",
    "    print(\"---NODE: RETRIEVE KB---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = retriever.invoke(question)\n",
    "    print(f\"Retrieved {len(documents)} documents.\")\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def generate_answer(state):\n",
    "    \"\"\"Generates an answer using the retrieved documents as context.\"\"\"\n",
    "    print(\"---NODE: GENERATE ANSWER---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    context = \"\\n\\n\".join([f\"Source ID: {doc.metadata['doc_id']}\\nContent: {doc.page_content}\" for doc in documents])\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a helpful assistant for question-answering tasks.\n",
    "        - Use only the provided context to answer the user's question.\n",
    "        - Your answer must be grounded in the facts from the snippets.\n",
    "        - For every claim or piece of information in your answer, you MUST cite the source using its 'Source ID' in the format [KBxxx].\n",
    "        - If the context does not contain the answer, state that you cannot answer the question.\"\"\"),\n",
    "        (\"human\", f\"Question: {question}\\n\\nContext:\\n{context}\")\n",
    "    ])\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    generation = chain.invoke({\"question\": question, \"context\": context})\n",
    "    print(\"Generated Answer:\\n\", generation)\n",
    "    return {\"generation\": generation}\n",
    "\n",
    "def critique_answer(state):\n",
    "    \"\"\"Critiques the generated answer for completeness and factual consistency.\"\"\"\n",
    "    print(\"---NODE: CRITIQUE ANSWER---\")\n",
    "    context = \"\\n\\n\".join([f\"Source ID: {doc.metadata['doc_id']}\\nContent: {doc.page_content}\" for doc in state[\"documents\"]])\n",
    "    critique_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are an expert critic. Your task is to evaluate a generated answer against the provided context.\n",
    "        - Check if the answer fully addresses the user's question based *only* on the information available in the context.\n",
    "        - Check for any hallucinations or information in the answer that is not supported by the context.\n",
    "        - If the answer is complete and factually grounded, respond with the single word: 'COMPLETE'.\n",
    "        - If the answer is lacking key details from the context or makes unsupported claims, respond with 'REFINE:' followed by a concise, comma-separated list of missing topics or keywords that need to be included.\"\"\"),\n",
    "        (\"human\", f\"User Question: {state['question']}\\n\\nContext:\\n{context}\\n\\nGenerated Answer:\\n{state['generation']}\")\n",
    "    ])\n",
    "    chain = critique_prompt | llm | StrOutputParser()\n",
    "    critique = chain.invoke({})\n",
    "    print(\"Critique Result:\", critique)\n",
    "    return {\"critique\": critique}\n",
    "\n",
    "def refine_answer(state):\n",
    "    \"\"\"Refines the answer by retrieving one more document and regenerating.\"\"\"\n",
    "    print(\"---NODE: REFINE ANSWER---\")\n",
    "    missing_keywords = state[\"critique\"].replace(\"REFINE:\", \"\").strip()\n",
    "    print(f\"Refining based on missing keywords: {missing_keywords}\")\n",
    "    refinement_retriever = vectorstore.as_retriever(search_kwargs={'k': 1})\n",
    "    new_doc = refinement_retriever.invoke(missing_keywords)\n",
    "    documents = state[\"documents\"]\n",
    "    documents.extend(new_doc)\n",
    "    state['documents'] = documents\n",
    "    refined_state = generate_answer(state)\n",
    "    return {\"generation\": refined_state[\"generation\"]}\n",
    "\n",
    "def decide_to_refine(state):\n",
    "    \"\"\"Decision logic to route to refinement or end the process.\"\"\"\n",
    "    print(\"---DECISION: CHECK CRITIQUE---\")\n",
    "    if state[\"critique\"].upper() == \"COMPLETE\":\n",
    "        print(\"Decision: COMPLETE. Ending graph.\")\n",
    "        return \"end\"\n",
    "    else:\n",
    "        print(\"Decision: REFINE. Proceeding to refinement.\")\n",
    "        return \"refine\"\n",
    "\n",
    "### ASSEMBLE THE GRAPH ###\n",
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node(\"retrieve_kb\", retrieve_kb)\n",
    "workflow.add_node(\"generate_answer\", generate_answer)\n",
    "workflow.add_node(\"critique_answer\", critique_answer)\n",
    "workflow.add_node(\"refine_answer\", refine_answer)\n",
    "\n",
    "workflow.set_entry_point(\"retrieve_kb\")\n",
    "workflow.add_edge(\"retrieve_kb\", \"generate_answer\")\n",
    "workflow.add_edge(\"generate_answer\", \"critique_answer\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"critique_answer\",\n",
    "    decide_to_refine,\n",
    "    {\"refine\": \"refine_answer\", \"end\": END},\n",
    ")\n",
    "workflow.add_edge(\"refine_answer\", END)\n",
    "\n",
    "agentic_rag_app = workflow.compile()\n",
    "print(\"‚úÖ LangGraph workflow compiled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing the Pipeline\n",
    "\n",
    "Finally, we run the test queries through our compiled agentic RAG application and print the final, citation-backed answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- üöÄ Running Test Queries ---\")\n",
    "\n",
    "test_queries = [\n",
    "    \"What are best practices for caching?\",\n",
    "    \"How should I set up CI/CD pipelines?\",\n",
    "    \"What are performance tuning tips?\",\n",
    "    \"How do I version my APIs?\",\n",
    "    \"What should I consider for error handling?\"\n",
    "]\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"\\n{'='*50}\\nEXECUTING QUERY {i}: '{query}'\\n{'='*50}\")\n",
    "    \n",
    "    inputs = {\"question\": query}\n",
    "    final_answer = \"\"\n",
    "    \n",
    "    # stream() allows us to see the outputs from each step as it runs\n",
    "    for output in agentic_rag_app.stream(inputs):\n",
    "        # The final answer is the last output from either the 'generate_answer' or 'refine_answer' node\n",
    "        for key, value in output.items():\n",
    "            if key == \"generate_answer\" or key == \"refine_answer\":\n",
    "                final_answer = value['generation']\n",
    "    \n",
    "    print(f\"\\n‚úÖ FINAL ANSWER FOR QUERY '{query}':\\n\\n{final_answer}\\n\")\n",
    "\n",
    "print(\"--- All queries processed. ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
